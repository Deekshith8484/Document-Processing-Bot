{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40688bdb8b4d42f6a1158b46eaf3b368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='File Path:', placeholder='Enter the path to the .docx file')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff92456bc3f945b9979c94f7ab15e792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Query:', placeholder='Enter your query')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc09d6430f24cbe968764a878ce1a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Process Document', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cebf560c3e345489a8feb14d2c1d484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document  # For .docx parsing\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Load the .docx file\n",
    "def load_docx(file_path):\n",
    "    \"\"\"Loads the .docx file and extracts text from all paragraphs.\"\"\"\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# Preprocess the text (Basic chunking and cleaning)\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Cleans the text by removing extra spaces and special characters.\"\"\"\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Chunk text for better processing (This avoids hallucinations by giving the model shorter chunks)\n",
    "def chunk_text(text, chunk_size=1000):\n",
    "    \"\"\"Splits the text into smaller chunks of defined size.\"\"\"\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Function to retrieve relevant chunks based on a query\n",
    "def retrieve_relevant_chunks(query, text_chunks):\n",
    "    \"\"\"Retrieves the most relevant chunks from the document based on the query using TF-IDF and cosine similarity.\"\"\"\n",
    "    vectorizer = TfidfVectorizer().fit_transform([query] + text_chunks)\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Calculate cosine similarity between the query and each chunk\n",
    "    cosine_similarities = cosine_similarity([vectors[0]], vectors[1:])[0]\n",
    "\n",
    "    # Return top 3 most relevant chunks to avoid overloading the model (Helps in mitigating hallucinations)\n",
    "    top_indices = cosine_similarities.argsort()[-3:][::-1]\n",
    "    relevant_chunks = [text_chunks[i] for i in top_indices]\n",
    "    \n",
    "    return \" \".join(relevant_chunks)\n",
    "\n",
    "# Load and Initialize the model\n",
    "def initialize_model():\n",
    "    \"\"\"Initializes the BART large CNN model for summarization.\"\"\"\n",
    "    model_name = \"facebook/bart-large-cnn\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "    return summarizer\n",
    "\n",
    "# Summarize the relevant chunks\n",
    "def summarize_chunks(summarizer, relevant_chunks):\n",
    "    \"\"\"Summarizes the relevant chunks using the initialized model.\"\"\"\n",
    "    return summarizer(relevant_chunks, max_length=200, min_length=50, do_sample=True)[0]['summary_text']\n",
    "\n",
    "# Mitigate hallucination using cross-referencing (Second mitigation technique)\n",
    "def cross_reference_summary(summary, text_chunks):\n",
    "    \"\"\"Cross-references the summary with the original chunks using cosine similarity to mitigate hallucination.\"\"\"\n",
    "    vectorizer = TfidfVectorizer().fit_transform([summary] + text_chunks)\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Recalculate cosine similarity between the summary and original chunks\n",
    "    cosine_similarities = cosine_similarity([vectors[0]], vectors[1:])[0]\n",
    "\n",
    "    # If the summary doesn't match any chunk with high confidence, flag it\n",
    "    if max(cosine_similarities) < 0.5:\n",
    "        return \"The generated summary may be inaccurate. Please cross-check the source.\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Structured function call to extract the effective date\n",
    "def extract_effective_date(text):\n",
    "    \"\"\"Extracts the effective date using regex from the document text.\"\"\"\n",
    "    # Regular expression to match effective date formats\n",
    "    date_pattern = r'(\\b(?:\\d{1,2}(?:st|nd|rd|th)?\\s+(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|' \\\n",
    "                   r'Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|' \\\n",
    "                    r'Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s+\\d{4})\\b|' \\\n",
    "                   r'\\b(?:\\d{4}-\\d{2}-\\d{2})\\b|' \\\n",
    "                   r'\\b(?:\\d{2}/\\d{2}/\\d{4})\\b)'\n",
    "    match = re.search(date_pattern, text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return \"Effective date not found in the document.\"\n",
    "\n",
    "# Main function to run summarization and date extraction\n",
    "def process_document(file_path, query):\n",
    "    \"\"\"Main function that processes the document and retrieves the relevant information based on the query.\"\"\"\n",
    "    # Load the .docx file\n",
    "    docx_text = load_docx(file_path)\n",
    "\n",
    "    # Preprocess and chunk the text\n",
    "    cleaned_text = preprocess_text(docx_text)\n",
    "    text_chunks = chunk_text(cleaned_text)\n",
    "\n",
    "    # Initialize the model\n",
    "    summarizer = initialize_model()\n",
    "\n",
    "    # Retrieve relevant chunks\n",
    "    relevant_chunks = retrieve_relevant_chunks(query, text_chunks)\n",
    "\n",
    "    # Summarize the chunks\n",
    "    summary = summarize_chunks(summarizer, relevant_chunks)\n",
    "\n",
    "    # Mitigate hallucination using cross-referencing\n",
    "    final_summary = cross_reference_summary(summary, text_chunks)\n",
    "\n",
    "    # Extract the effective date using function calling\n",
    "    effective_date = extract_effective_date(docx_text)\n",
    "\n",
    "    return final_summary, effective_date\n",
    "\n",
    "# Interactive Bot with IPyWidgets\n",
    "\n",
    "# Widgets for file path and query\n",
    "file_path_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter the path to the .docx file',\n",
    "    description='File Path:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "query_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your query',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Button to trigger processing\n",
    "process_button = widgets.Button(description=\"Process Document\")\n",
    "\n",
    "# Output area\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Function to handle button click\n",
    "def on_button_click(b):\n",
    "    with output_area:\n",
    "        clear_output()  # Clear previous output\n",
    "        file_path = file_path_widget.value\n",
    "        query = query_widget.value\n",
    "        if file_path and query:\n",
    "            try:\n",
    "                summary, effective_date = process_document(file_path, query)\n",
    "                print(f\"Summary of the document: {summary}\")\n",
    "                print(f\"Extracted Effective Date: {effective_date}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        else:\n",
    "            print(\"Please enter both the file path and query.\")\n",
    "\n",
    "# Bind the button click event\n",
    "process_button.on_click(on_button_click)\n",
    "\n",
    "# Display widgets and output area\n",
    "display(file_path_widget, query_widget, process_button, output_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
